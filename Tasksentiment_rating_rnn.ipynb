{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#6629b2'>Predicting sentiment ratings with RNN using Keras</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Study Material</font>\n",
    "- https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "- https://www.youtube.com/watch?v=UNmqTiOnRfg\n",
    "- https://www.youtube.com/watch?v=WCUNPb-5EYI\n",
    "- https://www.youtube.com/watch?v=OuYtk9Ymut4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Dataset</font>\n",
    "\n",
    "The Large Movie Review Datasetconsists of 50,000 movie reviews from [IMDB](http://www.imdb.com/). The ratings are on a 1-10 scale, but the dataset specifically contains \"polarized\" reviews: positive reviews with a rating of 7 or higher, and negative reviews with a rating of 4 or lower. There are an equal number of positive and negative reviews. \n",
    "\n",
    "You can download the dataset from: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load the dataset into variable \"reviews\". You can truncate the dataset to keep a few hundred records if it's\n",
    "    taking too long to process/train. Keep in mind, bigger the dataset, higher the accuracy score!\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "reviews = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_reviews, test_reviews, train_sentiment, test_sentiment = train_test_split(reviews[\"review\"],reviews[\"sentiment\"],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6596</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46322</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8238</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33572</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13501</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32531</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32214</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25171</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17671</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment\n",
       "6596           0\n",
       "46322          1\n",
       "8238           1\n",
       "33572          1\n",
       "19731          0\n",
       "...          ...\n",
       "13501          0\n",
       "32531          1\n",
       "32214          1\n",
       "25171          1\n",
       "17671          1\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the above train test datasets into a pandas dataframe. You shoud have 4 dataframes.\n",
    "# Name them train_reviews, test_reviews, train_sentiment, test_sentiment.\n",
    "# In train_sentiment and test_sentiment, convert \"positive\" to 1 and \"negative\" to 0\n",
    "\n",
    "train_reviews = pd.DataFrame(train_reviews)\n",
    "test_reviews = pd.DataFrame(test_reviews)\n",
    "train_sentiment = pd.DataFrame(train_sentiment)\n",
    "test_sentiment = pd.DataFrame(test_sentiment)\n",
    "\n",
    "train_reviews = train_reviews.head(100)\n",
    "test_reviews = test_reviews.head(100)\n",
    "train_sentiment = train_sentiment.head(100)\n",
    "test_sentiment = test_sentiment.head(100)\n",
    "\n",
    "def change(text):\n",
    "    if text == \"positive\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train_sentiment[\"sentiment\"] = train_sentiment[\"sentiment\"].apply(lambda x: change(x))\n",
    "train_sentiment\n",
    "test_sentiment[\"sentiment\"] = test_sentiment[\"sentiment\"].apply(lambda x: change(x))\n",
    "\n",
    "test_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28964</th>\n",
       "      <td>Seriously, Sci-Fi needs to stop making movies....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28393</th>\n",
       "      <td>wow...I just watched this movie...American peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34058</th>\n",
       "      <td>\"Go Fish\" garnered Rose Troche rightly or wron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31485</th>\n",
       "      <td>What a waste! This movie could have really bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23325</th>\n",
       "      <td>This movie is really bad, trying to create sci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review\n",
       "28964  Seriously, Sci-Fi needs to stop making movies....\n",
       "28393  wow...I just watched this movie...American peo...\n",
       "34058  \"Go Fish\" garnered Rose Troche rightly or wron...\n",
       "31485  What a waste! This movie could have really bee...\n",
       "23325  This movie is really bad, trying to create sci..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Preparing the data</font>\n",
    "\n",
    "###  <font color='#6629b2'>Tokenization</font>\n",
    "\n",
    "The first preprocessing step is to tokenize each of the reviews into (lowercased) individual words, since the models will encode the reviews at the word level (rather than subword units like characters, for example). For this we'll use [spaCy](https://spacy.io/), which is a fast and extremely user-friendly library that performs various language processing tasks. Once you load a spaCy model for a particular language, you can provide any text as input to the model, e.g. encoder(text) and access its linguistic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>Tokenized_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28964</th>\n",
       "      <td>Seriously, Sci-Fi needs to stop making movies....</td>\n",
       "      <td>[seriously, scifi, needs, to, stop, making, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28393</th>\n",
       "      <td>wow...I just watched this movie...American peo...</td>\n",
       "      <td>[wowi, just, watched, this, movieamerican, peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34058</th>\n",
       "      <td>\"Go Fish\" garnered Rose Troche rightly or wron...</td>\n",
       "      <td>[go, fish, garnered, rose, troche, rightly, or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31485</th>\n",
       "      <td>What a waste! This movie could have really bee...</td>\n",
       "      <td>[what, a, waste, this, movie, could, have, rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23325</th>\n",
       "      <td>This movie is really bad, trying to create sci...</td>\n",
       "      <td>[this, movie, is, really, bad, trying, to, cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26188</th>\n",
       "      <td>My 7-year-old daughter loved it, as Disney exe...</td>\n",
       "      <td>[my, 7yearold, daughter, loved, it, as, disney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>I am Curious (Yellow) (a film, in near Seussic...</td>\n",
       "      <td>[i, am, curious, yellow, a, film, in, near, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46671</th>\n",
       "      <td>This meandering tale of mob revenge is simply ...</td>\n",
       "      <td>[this, meandering, tale, of, mob, revenge, is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11180</th>\n",
       "      <td>This was my very first \"Bollywood\" movie and I...</td>\n",
       "      <td>[this, was, my, very, first, bollywood, movie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17570</th>\n",
       "      <td>After watching some of HBO's great stuff - Ban...</td>\n",
       "      <td>[after, watching, some, of, hbos, great, stuff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  \\\n",
       "28964  Seriously, Sci-Fi needs to stop making movies....   \n",
       "28393  wow...I just watched this movie...American peo...   \n",
       "34058  \"Go Fish\" garnered Rose Troche rightly or wron...   \n",
       "31485  What a waste! This movie could have really bee...   \n",
       "23325  This movie is really bad, trying to create sci...   \n",
       "26188  My 7-year-old daughter loved it, as Disney exe...   \n",
       "14387  I am Curious (Yellow) (a film, in near Seussic...   \n",
       "46671  This meandering tale of mob revenge is simply ...   \n",
       "11180  This was my very first \"Bollywood\" movie and I...   \n",
       "17570  After watching some of HBO's great stuff - Ban...   \n",
       "\n",
       "                                        Tokenized_Review  \n",
       "28964  [seriously, scifi, needs, to, stop, making, mo...  \n",
       "28393  [wowi, just, watched, this, movieamerican, peo...  \n",
       "34058  [go, fish, garnered, rose, troche, rightly, or...  \n",
       "31485  [what, a, waste, this, movie, could, have, rea...  \n",
       "23325  [this, movie, is, really, bad, trying, to, cre...  \n",
       "26188  [my, 7yearold, daughter, loved, it, as, disney...  \n",
       "14387  [i, am, curious, yellow, a, film, in, near, se...  \n",
       "46671  [this, meandering, tale, of, mob, revenge, is,...  \n",
       "11180  [this, was, my, very, first, bollywood, movie,...  \n",
       "17570  [after, watching, some, of, hbos, great, stuff...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Lowercase and tokenise all the reviews in train_reviews using spacy'''\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "encoder = spacy.load('en_core_web_sm')\n",
    "\n",
    "def text_to_tokens(text_seqs):\n",
    "    #complete this function that lowers and tokenizes the reviews\n",
    "    text_seqs = re.sub(r'[^\\w\\s]+',\"\",text_seqs)\n",
    "    doc = encoder(text_seqs)\n",
    "    l1 = []\n",
    "    for word in doc: \n",
    "        l1.append(word.text.lower())\n",
    "    return l1\n",
    "\n",
    "#train_reviews['Tokenized_Review'] = text_to_tokens(train_reviews['review'])\n",
    "train_reviews['Tokenized_Review'] = train_reviews['review'].apply(lambda x:text_to_tokens(x))\n",
    "test_reviews['Tokenized_Review'] = test_reviews['review'].apply(lambda x:text_to_tokens(x))\n",
    "\n",
    "train_reviews[['review','Tokenized_Review']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Lexicon</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Each tokenized word in the reviews is added to the lexicon, and then each word is mapped to a numerical index that can be read by the model. Since large datasets may contain a huge number of unique words, it's common to filter all words occurring less than a certain number of times, and replace them with some generic &lt;UNK&gt; token. The min_freq parameter in the function below defines this threshold. When assigning the indices, the number 1 will represent unknown words. The number 0 will represent \"empty\" word slots, which is explained below. Therefore \"real\" words will have indices of 2 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5': 2, 'acting': 3, 'after': 4, 'along': 5, 'at': 6, 'away': 7, 'brings': 8, 'by': 9, 'ca': 10, 'call': 11, 'cheesy': 12, 'could': 13, 'creature': 14, 'crew': 15, 'daughter': 16, 'dr': 17, 'either': 18, 'escapes': 19, 'figure': 20, 'from': 21, 'go': 22, 'guy': 23, 'had': 24, 'he': 25, 'help': 26, 'his': 27, 'horrible': 28, 'how': 29, 'however': 30, 'incredibly': 31, 'john': 32, 'kill': 33, 'killing': 34, 'looks': 35, 'making': 36, 'many': 37, 'men': 38, 'movies': 39, 'needs': 40, 'or': 41, 'other': 42, 'out': 43, 're': 44, 'scifi': 45, 'seriously': 46, 'ship': 47, 'shoot': 48, 'so': 49, 'some': 50, 'starts': 51, 'stay': 52, 'stop': 53, 'them': 54, 'then': 55, 'there': 56, 'they': 57, 'thing': 58, 'about': 59, 'also': 60, 'another': 61, 'black': 62, 'cinema': 63, 'cinematography': 64, 'content': 65, 'country': 66, 'cover': 67, 'crap': 68, 'dances': 69, 'days': 70, 'different': 71, 'direction': 72, 'director': 73, 'far': 74, 'films': 75, 'friday': 76, 'gangster': 77, 'good': 78, 'highly': 79, 'hindi': 80, 'hollywood': 81, 'india': 82, 'indian': 83, 'instead': 84, 'just': 85, 'like': 86, 'love': 87, 'more': 88, 'most': 89, 'mother': 90, 'much': 91, 'pathetic': 92, 'people': 93, 'piece': 94, 'real': 95, 'realistic': 96, 'recommended': 97, 'shows': 98, 'simply': 99, 'songs': 100, 'subjects': 101, 'such': 102, 'than': 103, 'these': 104, 'today': 105, 'touching': 106, 'towards': 107, 'truth': 108, 'variety': 109, 'very': 110, 'view': 111, 'watch': 112, 'watched': 113, 'western': 114, 'what': 115, 'where': 116, 'window': 117, 'actually': 118, 'attracted': 119, 'been': 120, 'both': 121, 'do': 122, 'entire': 123, 'even': 124, 'fish': 125, 'garnered': 126, 'hard': 127, 'has': 128, 'here': 129, 'human': 130, 'into': 131, 'know': 132, 'learned': 133, 'little': 134, 'look': 135, 'made': 136, 'nineties': 137, 'now': 138, 'portrayed': 139, 'revelation': 140, 'rightly': 141, 'screen': 142, 'sexually': 143, 'she': 144, 'supposed': 145, 'turn': 146, 'understand': 147, 'up': 148, 'us': 149, 'way': 150, 'well': 151, 'who': 152, 'whole': 153, 'women': 154, 'wrongly': 155, '70s': 156, 'basic': 157, 'big': 158, 'characters': 159, 'crafted': 160, 'deals': 161, 'decent': 162, 'great': 163, 'historical': 164, 'late': 165, 'main': 166, 'mike': 167, 'overall': 168, 'particular': 169, 'rather': 170, 'really': 171, 'same': 172, 'shallow': 173, 'similar': 174, 'something': 175, 'themes': 176, 'time': 177, 'uninteresting': 178, 'waste': 179, 'writing': 180, '100': 181, '50': 182, 'against': 183, 'air': 184, 'almost': 185, 'always': 186, 'any': 187, 'apparently': 188, 'back': 189, 'bad': 190, 'bastards': 191, 'because': 192, 'being': 193, 'book': 194, 'can': 195, 'chance': 196, 'coming': 197, 'create': 198, 'dating': 199, 'ends': 200, 'everybody': 201, 'eyes': 202, 'full': 203, 'gives': 204, 'going': 205, 'hair': 206, 'happens': 207, 'him': 208, 'history': 209, 'if': 210, 'important': 211, 'inside': 212, 'instance': 213, 'jokes': 214, 'kept': 215, 'kicked': 216, 'killed': 217, 'makes': 218, 'mention': 219, 'never': 220, 'notice': 221, 'only': 222, 'peoples': 223, 'person': 224, 'point': 225, 'probably': 226, 'remember': 227, 'someone': 228, 'survive': 229, 'taking': 230, 'things': 231, 'those': 232, 'trying': 233, 'walk': 234, 'watching': 235, 'were': 236, 'write': 237, 'yellow': 238, 'alone': 239, 'avoid': 240, 'before': 241, 'better': 242, 'child': 243, 'couple': 244, 'done': 245, 'enough': 246, 'fails': 247, 'general': 248, 'get': 249, 'home': 250, 'humor': 251, 'idea': 252, 'loved': 253, 'm': 254, 'meet': 255, 'meets': 256, 'my': 257, 'ones': 258, 'opinion': 259, 'original': 260, 'period': 261, 'politically': 262, 'problem': 263, 'rating': 264, 's': 265, 'seen': 266, 'standard': 267, 'story': 268, 'strictly': 269, 'think': 270, 've': 271, 'video': 272, 'which': 273, 'worst': 274, 'would': 275, 'your': 276, '40': 277, 'actors': 278, 'acts': 279, 'actual': 280, 'afraid': 281, 'am': 282, 'ambitious': 283, 'an': 284, 'analysis': 285, 'anyone': 286, 'around': 287, 'arthouse': 288, 'attempt': 289, 'available': 290, 'beautifully': 291, 'become': 292, 'behind': 293, 'bit': 294, 'blue': 295, 'boyfriend': 296, 'break': 297, 'car': 298, 'cards': 299, 'character': 300, 'class': 301, 'clichés': 302, 'close': 303, 'coherent': 304, 'comes': 305, 'comfortable': 306, 'comment': 307, 'completely': 308, 'confused': 309, 'considering': 310, 'court': 311, 'credits': 312, 'd': 313, 'directing': 314, 'documentary': 315, 'does': 316, 'down': 317, 'dramatic': 318, 'due': 319, 'during': 320, 'emotional': 321, 'end': 322, 'entirely': 323, 'everything': 324, 'experience': 325, 'experiment': 326, 'exploring': 327, 'father': 328, 'feel': 329, 'fill': 330, 'filmmaker': 331, 'first': 332, 'foreign': 333, 'fully': 334, 'funny': 335, 'generally': 336, 'getting': 337, 'girlfriend': 338, 'graphic': 339, 'having': 340, 'held': 341, 'her': 342, 'hey': 343, 'himself': 344, 'hits': 345, 'ideas': 346, 'impressive': 347, 'interesting': 348, 'ironically': 349, 'itself': 350, 'kid': 351, 'king': 352, 'knew': 353, 'last': 354, 'later': 355, 'least': 356, 'leave': 357, 'less': 358, 'lesser': 359, 'looking': 360, 'major': 361, 'make': 362, 'may': 363, 'maybe': 364, 'means': 365, 'midst': 366, 'midway': 367, 'might': 368, 'mix': 369, 'mostly': 370, 'narration': 371, 'narrative': 372, 'near': 373, 'nudity': 374, 'occasional': 375, 'off': 376, 'once': 377, 'outstanding': 378, 'part': 379, 'performance': 380, 'perhaps': 381, 'phenomenon': 382, 'picture': 383, 'playing': 384, 'politics': 385, 'print': 386, 'pure': 387, 'purely': 388, 'put': 389, 'quite': 390, 'relationship': 391, 'right': 392, 'rock': 393, 'said': 394, 'satire': 395, 'say': 396, 'scenes': 397, 'see': 398, 'seems': 399, 'serious': 400, 'sex': 401, 'shooting': 402, 'should': 403, 'shown': 404, 'since': 405, 'slightly': 406, 'sort': 407, 'stands': 408, 'start': 409, 'state': 410, 'statement': 411, 'structure': 412, 'subtle': 413, 'successful': 414, 'supreme': 415, 'surprise': 416, 'technique': 417, 'their': 418, 'through': 419, 'title': 420, 'toes': 421, 'too': 422, 'true': 423, 'two': 424, 'use': 425, 'used': 426, 'usually': 427, 'viewer': 428, 'vulnerable': 429, 'wars': 430, 'ways': 431, 'we': 432, 'went': 433, 'whatever': 434, 'when': 435, 'while': 436, 'will': 437, 'woods': 438, 'years': 439, 'yet': 440, 'again': 441, 'badly': 442, 'bring': 443, 'brown': 444, 'cable': 445, 'cop': 446, 'dull': 447, 'effectively': 448, 'heavy': 449, 'james': 450, 'jim': 451, 'kicks': 452, 'life': 453, 'plays': 454, 'revenge': 455, 'role': 456, 'roles': 457, 'score': 458, 'still': 459, ' ': 460, '3': 461, 'add': 462, 'amazing': 463, 'bollywood': 464, 'come': 465, 'comedy': 466, 'contrast': 467, 'dance': 468, 'dancing': 469, 'described': 470, 'did': 471, 'english': 472, 'everyone': 473, 'exhausted': 474, 'favorite': 475, 'find': 476, 'found': 477, 'fresh': 478, 'fun': 479, 'gave': 480, 'happen': 481, 'horror': 482, 'ill': 483, 'industry': 484, 'job': 485, 'join': 486, 'kitty': 487, 'laughter': 488, 'let': 489, 'live': 490, 'me': 491, 'miss': 492, 'murder': 493, 'myself': 494, 'new': 495, 'normal': 496, 'nothing': 497, 'number': 498, 'over': 499, 'party': 500, 'plot': 501, 'pretty': 502, 'reading': 503, 'recent': 504, 'romance': 505, 'rough': 506, 'rules': 507, 'second': 508, 'set': 509, 'somewhat': 510, 'take': 511, 'total': 512, 'translation': 513, 'viewers': 514, 'want': 515, 'week': 516, 'whoever': 517, 'why': 518, 'world': 519, 'actor': 520, 'annoying': 521, 'average': 522, 'band': 523, 'best': 524, 'bill': 525, 'blood': 526, 'brother': 527, 'brothers': 528, 'budget': 529, 'cares': 530, 'channel': 531, 'comments': 532, 'definitely': 533, 'deliver': 534, 'disappointed': 535, 'disgusting': 536, 'else': 537, 'embarrassing': 538, 'episode': 539, 'etc': 540, 'expectations': 541, 'expected': 542, 'extremely': 543, 'family': 544, 'few': 545, 'filmed': 546, 'finally': 547, 'finds': 548, 'gets': 549, 'girl': 550, 'give': 551, 'goes': 552, 'gone': 553, 'guys': 554, 'hbos': 555, 'hear': 556, 'high': 557, 'honestly': 558, 'horribly': 559, 'intriguing': 560, 'joke': 561, 'kind': 562, 'level': 563, 'lightly': 564, 'low': 565, 'magic': 566, 'mean': 567, 'mediocre': 568, 'missed': 569, 'modern': 570, 'moments': 571, 'must': 572, 'mysterious': 573, 'neck': 574, 'nobody': 575, 'often': 576, 'okay': 577, 'our': 578, 'parts': 579, 'performances': 580, 'plan': 581, 'poorly': 582, 'powers': 583, 'predictable': 584, 'presented': 585, 'quality': 586, 'radio': 587, 'regular': 588, 'ridiculous': 589, 'scary': 590, 'scene': 591, 'script': 592, 'seconds': 593, 'seem': 594, 'seemed': 595, 'shame': 596, 'short': 597, 'show': 598, 'soap': 599, 'society': 600, 'speak': 601, 'stephen': 602, 'struggle': 603, 'stuff': 604, 'teen': 605, 'television': 606, 'though': 607, 'thought': 608, 'throws': 609, 'totally': 610, 'tv': 611, 'unconvincing': 612, 'unfortunately': 613, 'wanted': 614, 'watches': 615, 'without': 616, 'written': 617, 'wrong': 618, 'wrote': 619, 'crime': 620, 'entertaining': 621, 'excellent': 622, 'expecting': 623, 'got': 624, 'hate': 625, 'liked': 626, 'pace': 627, 'pleasantly': 628, 'portrayals': 629, 'reviews': 630, 'small': 631, 'surprised': 632, '10': 633, 'accent': 634, 'awards': 635, 'beyond': 636, 'case': 637, 'context': 638, 'course': 639, 'description': 640, 'ever': 641, 'every': 642, 'fast': 643, 'female': 644, 'friend': 645, 'hilarious': 646, 'laugh': 647, 'line': 648, 'meant': 649, 'minutes': 650, 'moment': 651, 'need': 652, 'perfect': 653, 'pops': 654, 'porn': 655, 'production': 656, 'recommend': 657, 'softcore': 658, 'straight': 659, 'suppose': 660, 'terrible': 661, 'thriller': 662, 'thrillers': 663, 'win': 664, 'woman': 665, 'absolute': 666, 'based': 667, 'certainly': 668, 'childish': 669, 'dvd': 670, 'enter': 671, 'events': 672, 'explained': 673, 'explains': 674, 'famous': 675, 'fictional': 676, 'forgot': 677, 'front': 678, 'happened': 679, 'impression': 680, 'interest': 681, 'kinda': 682, 'living': 683, 'loyalty': 684, 'member': 685, 'mentioned': 686, 'no': 687, 'public': 688, 'read': 689, 'reason': 690, 'saw': 691, 'son': 692, 'soon': 693, 'stars': 694, 'started': 695, 'stick': 696, 'tongue': 697, 'word': 698, 'wore': 699, 'young': 700, 'adult': 701, 'already': 702, 'copy': 703, 'dimension': 704, 'entertainment': 705, 'heaven': 706, 'hell': 707, 'imagined': 708, 'local': 709, 'remain': 710, 'rent': 711, 'rented': 712, 'seeing': 713, 'sin': 714, 'speaking': 715, 'suggest': 716, 'unless': 717, 'wicked': 718, 'worth': 719, 'yes': 720, 'author': 721, 'between': 722, 'chemistry': 723, 'college': 724, 'depicting': 725, 'inspired': 726, 'man': 727, 'marriage': 728, 'memories': 729, 'wonderful': 730, '90s': 731, 'bodies': 732, 'bunch': 733, 'day': 734, 'fact': 735, 'husband': 736, 'lazy': 737, 'lost': 738, 'old': 739, 'past': 740, 'rich': 741, 'sadly': 742, 'students': 743, 'together': 744, 'wife': 745, '1999': 746, '2': 747, 'awful': 748, 'called': 749, 'ending': 750, 'grace': 751, 'rest': 752, 'showed': 753, 'situations': 754, '80s': 755, 'audience': 756, 'beat': 757, 'blind': 758, 'blonde': 759, 'body': 760, 'brief': 761, 'career': 762, 'doing': 763, 'familiar': 764, 'fine': 765, 'flashback': 766, 'given': 767, 'initial': 768, 'lewis': 769, 'mind': 770, 'noir': 771, 'obviously': 772, 'own': 773, 'police': 774, 'premise': 775, 'question': 776, 'rain': 777, 'report': 778, 'seeming': 779, 'sense': 780, 'stand': 781, 'station': 782, 'style': 783, 'sure': 784, 'tells': 785, 'tension': 786, 'told': 787, 'undoubtedly': 788, 'using': 789, 'usual': 790, 'venetian': 791, 'waybr': 792, 'welcome': 793, 'white': 794, 'action': 795, 'breasts': 796, 'cheated': 797, 'five': 798, 'gore': 799, 'letdown': 800, 'naked': 801, 'prepared': 802, 'strong': 803, 'tepid': 804, 'brilliant': 805, 'camera': 806, 'frustrating': 807, 'leading': 808, 'memory': 809, 'necessary': 810, 'result': 811, '1': 812, '4': 813, 'absolutely': 814, 'appearance': 815, 'began': 816, 'betterbr': 817, 'club': 818, 'countries': 819, 'curse': 820, 'decision': 821, 'development': 822, 'dies': 823, 'door': 824, 'exactly': 825, 'example': 826, 'explain': 827, 'explaining': 828, 'fat': 829, 'fight': 830, 'fit': 831, 'friends': 832, 'head': 833, 'jump': 834, 'killer': 835, 'lady': 836, 'left': 837, 'lines': 838, 'location': 839, 'long': 840, 'loud': 841, 'luckily': 842, 'michael': 843, 'moving': 844, 'music': 845, 'none': 846, 'nowhere': 847, 'oh': 848, 'pop': 849, 'quiet': 850, 'rage': 851, 'scare': 852, 'scared': 853, 'sequences': 854, 'sorry': 855, 'sound': 856, 'tell': 857, 'terms': 858, 'terribly': 859, 'theatre': 860, 'three': 861, 'trailers': 862, 'twist': 863, 'unlike': 864, 'ups': 865, 'vertigo': 866, 'wait': 867, 'wo': 868, 'worse': 869, 'disappointment': 870, 'lacks': 871, 'money': 872, 'onedimensional': 873, 'originality': 874, 'wonder': 875, 'awesome': 876, 'play': 877, 'run': 878, 'several': 879, 'soundtrack': 880, 'timebr': 881, 'whom': 882, 'ya': 883, 'anything': 884, 'believe': 885, 'confuse': 886, 'dark': 887, 'hoping': 888, 'howard': 889, 'lame': 890, 'pacing': 891, 'save': 892, 'served': 893, 'tone': 894, 'waiting': 895, 'work': 896, '1010': 897, 'alright': 898, 'amanda': 899, 'american': 900, 'assassin': 901, 'bland': 902, 'cast': 903, 'code': 904, 'company': 905, 'complex': 906, 'complicated': 907, 'confusion': 908, 'consider': 909, 'critical': 910, 'critics': 911, 'cross': 912, 'da': 913, 'damon': 914, 'dead': 915, 'death': 916, 'despite': 917, 'directed': 918, 'double': 919, 'early': 920, 'enjoy': 921, 'enjoyable': 922, 'enjoyed': 923, 'eventually': 924, 'fall': 925, 'field': 926, 'finish': 927, 'form': 928, 'george': 929, 'group': 930, 'half': 931, 'happening': 932, 'havoc': 933, 'hour': 934, 'intelligent': 935, 'intense': 936, 'justice': 937, 'lately': 938, 'law': 939, 'lying': 940, 'mans': 941, 'matt': 942, 'mess': 943, 'message': 944, 'mindless': 945, 'oscar': 946, 'perform': 947, 'problems': 948, 'rave': 949, 'screenplay': 950, 'suspects': 951, 'teens': 952, 'thinking': 953, 'top': 954, 'tried': 955, 'turned': 956, 'upon': 957, 'wants': 958, 'writer': 959, 'admit': 960, 'begin': 961, 'brain': 962, 'culture': 963, 'dialogue': 964, 'easy': 965, 'giving': 966, 'grade': 967, 'knows': 968, 'others': 969, 'packed': 970, 'sit': 971, 'sixth': 972, 'ten': 973, 'abrupt': 974, 'acted': 975, 'actress': 976, 'ago': 977, 'appeared': 978, 'appears': 979, 'believable': 980, 'born': 981, 'bought': 982, 'boy': 983, 'came': 984, 'caring': 985, 'certain': 986, 'change': 987, 'david': 988, 'decided': 989, 'duration': 990, 'excels': 991, 'fan': 992, 'fans': 993, 'favourite': 994, 'final': 995, 'flaws': 996, 'focus': 997, 'foot': 998, 'franchise': 999, 'god': 1000, 'herself': 1001, 'highlight': 1002, 'highlights': 1003, 'learn': 1004, 'lot': 1005, 'majority': 1006, 'met': 1007, 'months': 1008, 'moviebr': 1009, 'mr': 1010, 'nasty': 1011, 'nearly': 1012, 'nice': 1013, 'night': 1014, 'numerous': 1015, 'pieces': 1016, 'previous': 1017, 'running': 1018, 'scenario': 1019, 'sequence': 1020, 'series': 1021, 'slick': 1022, 'special': 1023, 'spoil': 1024, 'strict': 1025, 'superb': 1026, 'superbly': 1027, 'superior': 1028, 'supporting': 1029, 'takes': 1030, 'thrilling': 1031, 'throughout': 1032, 'times': 1033, 'trilogy': 1034, 'truly': 1035, 'try': 1036, 'turns': 1037, 'ultimatum': 1038, 'until': 1039, 'violent': 1040, 'york': 1041, 'yourself': 1042, 'attempts': 1043, 'cardboard': 1044, 'comic': 1045, 'creativity': 1046, 'details': 1047, 'effect': 1048, 'heard': 1049, 'humanity': 1050, 'inane': 1051, 'laughable': 1052, 'master': 1053, 'name': 1054, 'neither': 1055, 'plain': 1056, 'shots': 1057, 'side': 1058, 'watchable': 1059, '1993': 1060, 'courage': 1061, 'friendship': 1062, 'largely': 1063, 'south': 1064, 'steven': 1065, '13th': 1066, 'act': 1067, 'banal': 1068, 'boring': 1069, 'carrie': 1070, 'choice': 1071, 'dialog': 1072, 'die': 1073, 'disabled': 1074, 'dumb': 1075, 'fighting': 1076, 'hackneyed': 1077, 'halloween': 1078, 'including': 1079, 'jeering': 1080, 'junk': 1081, 'lets': 1082, 'll': 1083, 'mentally': 1084, 'missing': 1085, 'obsessed': 1086, 'obvious': 1087, 'outbr': 1088, 'perfection': 1089, 'place': 1090, 'realize': 1091, 'room': 1092, 'scream': 1093, 'single': 1094, 'stupid': 1095, 'talking': 1096, 'throat': 1097, 'twists': 1098, 'basically': 1099, 'describing': 1100, 'felt': 1101, 'hammer': 1102, 'hours': 1103, 'huge': 1104, 'sledge': 1105, '8': 1106, 'beloved': 1107, 'elements': 1108, 'evening': 1109, 'eye': 1110, 'flashed': 1111, 'seat': 1112, 'sight': 1113, 'sister': 1114, 'somehow': 1115, 'beginning': 1116, 'catch': 1117, 'cheap': 1118, 'children': 1119, 'crappy': 1120, 'destroy': 1121, 'difference': 1122, 'effects': 1123, 'glad': 1124, 'guess': 1125, 'incredible': 1126, 'jaws': 1127, 'parents': 1128, 'protected': 1129, 'shot': 1130, 'took': 1131, 'able': 1132, 'cary': 1133, 'cinematic': 1134, 'dated': 1135, 'died': 1136, 'engaging': 1137, 'facial': 1138, 'forever': 1139, 'grant': 1140, 'marshall': 1141, 'memorable': 1142, 'naturally': 1143, 'plus': 1144, 'shocking': 1145, 'voice': 1146, 'artificial': 1147, 'direct': 1148, 'equally': 1149, 'filmmakers': 1150, 'irritating': 1151, 'khan': 1152, 'list': 1153, 'outdated': 1154, 'outrageously': 1155, 'shock': 1156, 'weird': 1157, 'worked': 1158, 'year': 1159, 'anybody': 1160, 'classic': 1161, 'complete': 1162, 'doubt': 1163, 'except': 1164, 'expect': 1165, 'follow': 1166, 'leo': 1167, 'lowbrow': 1168, 'pain': 1169, 'remains': 1170, 'shakespeares': 1171, 'silly': 1172, 'spirit': 1173, 'surprisingly': 1174, 'tries': 1175, 'version': 1176, 'viewed': 1177, 'visuals': 1178, 'wesley': 1179, 'anywhere': 1180, 'bother': 1181, 'fantastic': 1182, 'fiennes': 1183, 'interested': 1184, 'paying': 1185, 'pointless': 1186, 'appreciate': 1187, 'box': 1188, 'chew': 1189, 'chewing': 1190, 'chilling': 1191, 'city': 1192, 'effective': 1193, 'heart': 1194, 'nicely': 1195, 'pacino': 1196, 'required': 1197, 'scenery': 1198, 'smart': 1199, 'contributes': 1200, 'dog': 1201, 'especially': 1202, 'filmbr': 1203, 'genuine': 1204, 'lead': 1205, 'mistakes': 1206, 'mood': 1207, 'odd': 1208, 'alcohol': 1209, 'circumstances': 1210, 'describes': 1211, 'growing': 1212, 'married': 1213, 'places': 1214, 'popular': 1215, 'awkward': 1216, 'convincing': 1217, 'de': 1218, 'elsewhere': 1219, 'exception': 1220, 'fantasy': 1221, 'hand': 1222, 'plastic': 1223, 'released': 1224, 'space': 1225, 'under': 1226, 'charming': 1227, 'faces': 1228, 'mainly': 1229, 'simple': 1230, 'strange': 1231, 'teambr': 1232, 'worthwhile': 1233, 'bloody': 1234, 'phone': 1235, 'school': 1236, 'understood': 1237, 'albeit': 1238, 'beholder': 1239, 'besides': 1240, 'contemporary': 1241, 'intended': 1242, 'leaves': 1243, 'misses': 1244, 'position': 1245, 'protagonists': 1246, 'resolution': 1247, 'stage': 1248, 'violence': 1249, 'above': 1250, 'although': 1251, 'barely': 1252, 'effort': 1253, 'fare': 1254, 'imagine': 1255, 'intelligence': 1256, 'produce': 1257, 'thank': 1258, 'type': 1259, 'battle': 1260, 'cgi': 1261, 'charge': 1262, 'favorites': 1263, 'insane': 1264, 'overthetop': 1265, 'personal': 1266, 'trick': 1267, 'whether': 1268, '20': 1269, 'attack': 1270, 'boat': 1271, 'creepy': 1272, 'easily': 1273, 'emotions': 1274, 'genre': 1275, 'greatly': 1276, 'keep': 1277, 'knowing': 1278, 'opening': 1279, 'situation': 1280, 'trapped': 1281, 'trees': 1282, '1989': 1283, '1994': 1284, '810': 1285, 'al': 1286, 'autobiography': 1287, 'award': 1288, 'control': 1289, 'daniel': 1290, 'difficult': 1291, 'dustin': 1292, 'emotionally': 1293, 'evidence': 1294, 'failed': 1295, 'foul': 1296, 'grim': 1297, 'hoffman': 1298, 'jodie': 1299, 'language': 1300, 'madness': 1301, 'manipulative': 1302, 'movement': 1303, 'next': 1304, 'novelist': 1305, 'played': 1306, 'political': 1307, 'portrayal': 1308, 'positive': 1309, 'robert': 1310, 'robin': 1311, 'round': 1312, 'triumph': 1313, 'williams': 1314, 'columbo': 1315, 'compelling': 1316, 'desire': 1317, 'drama': 1318, 'pulled': 1319, 'reality': 1320, 'self': 1321, 'sounds': 1322, 'tiny': 1323, 'villains': 1324, 'becomes': 1325, 'conform': 1326, 'excessive': 1327, 'nonsense': 1328, 'scientist': 1329, 'step': 1330, 'uses': 1331, 'lots': 1332, 'richard': 1333, 'anyway': 1334, 'attractions': 1335, 'baby': 1336, 'disagree': 1337, 'focuses': 1338, 'greatest': 1339, 'includes': 1340, 'century': 1341, 'citizens': 1342, 'explosions': 1343, 'images': 1344, 'lives': 1345, 'mankind': 1346, 'media': 1347, 'million': 1348, 'nature': 1349, 'offers': 1350, 'powerful': 1351, 'return': 1352, 'soldiers': 1353, 'free': 1354, 'makers': 1355, 'potential': 1356, 'thomas': 1357, 'thrill': 1358, 'abandoned': 1359, 'aside': 1360, 'boys': 1361, 'extreme': 1362, 'follows': 1363, 'four': 1364, 'garbage': 1365, 'genres': 1366, 'ground': 1367, 'hurt': 1368, 'leads': 1369, 'mad': 1370, 'masters': 1371, 'middle': 1372, 'photography': 1373, 'pretending': 1374, 'shadow': 1375, 'storytelling': 1376, 'stunt': 1377, 'taken': 1378, 'third': 1379, 'weak': 1380, 'added': 1381, 'affair': 1382, 'angles': 1383, 'animation': 1384, 'appearing': 1385, 'christian': 1386, 'detail': 1387, 'detective': 1388, 'each': 1389, 'frank': 1390, 'future': 1391, 'hit': 1392, 'investigating': 1393, 'jonathan': 1394, 'masterpiece': 1395, 'minority': 1396, 'ominous': 1397, 'presence': 1398, 'scenesbr': 1399, 'serve': 1400, 'stunning': 1401, 'stylish': 1402, 'tremendous': 1403, 'visually': 1404, 'matrix': 1405, 'fx': 1406, 'onebr': 1407, 'background': 1408, 'depth': 1409, 'eitherbr': 1410, 'entertained': 1411, 'lack': 1412, 'strangers': 1413, 'answer': 1414, 'beating': 1415, 'bridge': 1416, 'disjointed': 1417, 'dude': 1418, 'grows': 1419, 'hands': 1420, 'involved': 1421, 'logic': 1422, 'perfectly': 1423, 'psychology': 1424, 'speaks': 1425, 'store': 1426, 'unable': 1427, 'wish': 1428, 'amazingly': 1429, 'hokey': 1430, 'promised': 1431, 'stellar': 1432, 'ability': 1433, 'ambiance': 1434, 'creates': 1435, 'duvall': 1436, 'features': 1437, 'jack': 1438, 'puts': 1439, 'seven': 1440, 'star': 1441, 'associated': 1442, 'itbr': 1443, 'sequel': 1444, 'amount': 1445, 'beauty': 1446, 'energetic': 1447, 'performancebr': 1448, 'persona': 1449, 'research': 1450, 'sold': 1451, 'tragic': 1452, 'blame': 1453, 'illadvised': 1454, 'impossible': 1455, 'judged': 1456, 'na': 1457, 'named': 1458, 'says': 1459, 'surely': 1460, 'team': 1461, 'track': 1462, 'alive': 1463, 'art': 1464, 'beautiful': 1465, 'briefly': 1466, 'caricatures': 1467, 'chapter': 1468, 'concept': 1469, 'cool': 1470, 'costumes': 1471, 'deadbr': 1472, 'demonic': 1473, 'design': 1474, 'escape': 1475, 'feels': 1476, 'flick': 1477, 'green': 1478, 'hold': 1479, 'kids': 1480, 'otherwise': 1481, 'passed': 1482, 'poor': 1483, 'questionbr': 1484, 'ran': 1485, 'rod': 1486, 'setting': 1487, 'stolen': 1488, 'suicide': 1489, 'themselves': 1490, 'therefore': 1491, 'vs': 1492, 'weeks': 1493, 'check': 1494, 'makeup': 1495, 'sad': 1496, 'attractive': 1497, 'camerawork': 1498, 'claustrophobic': 1499, 'evil': 1500, 'fortunately': 1501, 'freaky': 1502, 'light': 1503, 'offed': 1504, 'turning': 1505, 'earth': 1506, 'eric': 1507, 'moved': 1508, 'rare': 1509, 'surprises': 1510, 'william': 1511, 'army': 1512, 'attention': 1513, 'existence': 1514, 'impressed': 1515, 'asked': 1516, 'care': 1517, 'dry': 1518, 'girls': 1519, 'heck': 1520, 'responsible': 1521, 'town': 1522, 'according': 1523, 'books': 1524, 'enjoys': 1525, 'face': 1526, 'henry': 1527, 'horrific': 1528, 'isolated': 1529, 'possibly': 1530, 'socalled': 1531, 'subject': 1532, 'theory': 1533, 'todays': 1534, 'finished': 1535, 'peter': 1536, 'charlie': 1537, 'charm': 1538, 'happy': 1539, 'marginalized': 1540, 'activities': 1541, 'adaptation': 1542, 'amateurish': 1543, 'dialogues': 1544, 'falls': 1545, 'jo': 1546, 'jr': 1547, 'laughing': 1548, 'order': 1549, 'working': 1550, 'conclusion': 1551, 'corny': 1552, 'alien': 1553, 'encounters': 1554, 'mission': 1555, 'hope': 1556, 'interpretation': 1557, 'mark': 1558, 'ruins': 1559, 'chose': 1560, 'hysterical': 1561, 'sharp': 1562, 'cut': 1563, 'laughed': 1564, 'wedding': 1565, 'minute': 1566, 'singer': 1567, 'blows': 1568, 'curiosity': 1569, 'among': 1570, 'andor': 1571, 'provoking': 1572, '<UNK>': 1}\n",
      "LEXICON SAMPLE (1572 total items):\n",
      "{'5': 2, 'acting': 3, 'after': 4, 'along': 5, 'at': 6, 'away': 7, 'brings': 8, 'by': 9, 'ca': 10, 'call': 11, 'cheesy': 12, 'could': 13, 'creature': 14, 'crew': 15, 'daughter': 16, 'dr': 17, 'either': 18, 'escapes': 19, 'figure': 20, 'from': 21}\n"
     ]
    }
   ],
   "source": [
    "'''Count tokens (words) in texts and add them to the lexicon'''\n",
    "\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def make_lexicon(token_seqs, min_freq=1, use_padding=False):\n",
    "    # First, count how often each word appears in the text. Save this count in a dictionary called token_counts\n",
    "    token_counts = {}\n",
    "    dictionary = gensim.corpora.Dictionary(train_reviews[\"Tokenized_Review\"])\n",
    "    \n",
    "    \n",
    "    # Then, assign each word to a numerical index, i.e save  all these words in a list. Filter words that occur less than or equal to min_freq times.\n",
    "    dictionary.filter_extremes(no_below = min_freq+1)\n",
    "    token_counts = dictionary.cfs\n",
    "    lexicon = dictionary\n",
    "\n",
    "    # lexicon = \n",
    "    \n",
    "    #create a dictionary lexicon that maps each word to its index. Note that indexes will start from 2,  index 0 is saved for padding and index 1 for unknown words ('<UNK>')\n",
    "    # lexicon = \n",
    "\n",
    "    list1 = []\n",
    "    for k,v in lexicon.iteritems():\n",
    "        list1.append((v,k+2))\n",
    "\n",
    "    dictionary = dict(list1)\n",
    "    lexicon = dictionary\n",
    "    \n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "    print(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(lexicon_size))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "lexicon = make_lexicon(token_seqs=train_reviews['Tokenized_Review'], min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>From strings to numbers</font>\n",
    "\n",
    "Once the lexicon is built, we can use it to transform each review from a list of string tokens into a list of numerical indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Review</th>\n",
       "      <th>Review_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28964</th>\n",
       "      <td>[seriously, scifi, needs, to, stop, making, mo...</td>\n",
       "      <td>[46, 45, 40, 53, 36, 39, 57, 44, 28, 24, 32, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28393</th>\n",
       "      <td>[wowi, just, watched, this, movieamerican, peo...</td>\n",
       "      <td>[85, 113, 93, 111, 107, 80, 75, 102, 83, 75, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34058</th>\n",
       "      <td>[go, fish, garnered, rose, troche, rightly, or...</td>\n",
       "      <td>[22, 125, 126, 141, 41, 155, 91, 55, 127, 147,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31485</th>\n",
       "      <td>[what, a, waste, this, movie, could, have, rea...</td>\n",
       "      <td>[115, 179, 13, 171, 120, 175, 162, 180, 169, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23325</th>\n",
       "      <td>[this, movie, is, really, bad, trying, to, cre...</td>\n",
       "      <td>[171, 190, 233, 198, 186, 200, 148, 230, 7, 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26188</th>\n",
       "      <td>[my, 7yearold, daughter, loved, it, as, disney...</td>\n",
       "      <td>[257, 16, 253, 144, 275, 265, 263, 184, 269, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>[i, am, curious, yellow, a, film, in, near, se...</td>\n",
       "      <td>[282, 238, 373, 394, 392, 6, 409, 290, 424, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46671</th>\n",
       "      <td>[this, meandering, tale, of, mob, revenge, is,...</td>\n",
       "      <td>[455, 99, 110, 348, 124, 456, 449, 451, 444, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11180</th>\n",
       "      <td>[this, was, my, very, first, bollywood, movie,...</td>\n",
       "      <td>[257, 110, 332, 464, 477, 172, 150, 37, 42, 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17570</th>\n",
       "      <td>[after, watching, some, of, hbos, great, stuff...</td>\n",
       "      <td>[4, 235, 50, 555, 163, 604, 460, 523, 528, 540...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Tokenized_Review  \\\n",
       "28964  [seriously, scifi, needs, to, stop, making, mo...   \n",
       "28393  [wowi, just, watched, this, movieamerican, peo...   \n",
       "34058  [go, fish, garnered, rose, troche, rightly, or...   \n",
       "31485  [what, a, waste, this, movie, could, have, rea...   \n",
       "23325  [this, movie, is, really, bad, trying, to, cre...   \n",
       "26188  [my, 7yearold, daughter, loved, it, as, disney...   \n",
       "14387  [i, am, curious, yellow, a, film, in, near, se...   \n",
       "46671  [this, meandering, tale, of, mob, revenge, is,...   \n",
       "11180  [this, was, my, very, first, bollywood, movie,...   \n",
       "17570  [after, watching, some, of, hbos, great, stuff...   \n",
       "\n",
       "                                             Review_Idxs  \n",
       "28964  [46, 45, 40, 53, 36, 39, 57, 44, 28, 24, 32, 2...  \n",
       "28393  [85, 113, 93, 111, 107, 80, 75, 102, 83, 75, 6...  \n",
       "34058  [22, 125, 126, 141, 41, 155, 91, 55, 127, 147,...  \n",
       "31485  [115, 179, 13, 171, 120, 175, 162, 180, 169, 6...  \n",
       "23325  [171, 190, 233, 198, 186, 200, 148, 230, 7, 21...  \n",
       "26188  [257, 16, 253, 144, 275, 265, 263, 184, 269, 2...  \n",
       "14387  [282, 238, 373, 394, 392, 6, 409, 290, 424, 23...  \n",
       "46671  [455, 99, 110, 348, 124, 456, 449, 451, 444, 4...  \n",
       "11180  [257, 110, 332, 464, 477, 172, 150, 37, 42, 50...  \n",
       "17570  [4, 235, 50, 555, 163, 604, 460, 523, 528, 540...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Convert each review from a list of tokens to a list of numbers (indices)'''\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon): \n",
    "    #complete this function to return a list of indexed tokens \n",
    "    id = []\n",
    "    for x in token_seqs:\n",
    "        for key,value in lexicon.items():\n",
    "            if x == key:\n",
    "                id.append(value)\n",
    "    idx_seqs = id\n",
    "    return idx_seqs\n",
    "\n",
    "#train_reviews['Review_Idxs'] = tokens_to_idxs(token_seqs=train_reviews['Tokenized_Review'], lexicon=lexicon)\n",
    "train_reviews['Review_Idxs'] = train_reviews['Tokenized_Review'].apply(lambda y:tokens_to_idxs(y,lexicon))\n",
    "test_reviews['Review_Idxs'] = test_reviews['Tokenized_Review'].apply(lambda y:tokens_to_idxs(y,lexicon))\n",
    "                                   \n",
    "train_reviews[['Tokenized_Review', 'Review_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INPUT:\n",
      " 28964    [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, ...\n",
      "28393    [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...\n",
      "34058    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...\n",
      "31485    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
      "23325    [0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, ...\n",
      "                               ...                        \n",
      "19051    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "17731    [0, 0, 0, 1, 0, 1, 2, 0, 0, 3, 0, 0, 0, 0, 0, ...\n",
      "48755    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "8748     [0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
      "30569    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: Review_Idxs, Length: 100, dtype: object\n",
      "SHAPE: (100,) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acting</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>after</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>real</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>realistic</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 2, 0, 0, 3, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>recommended</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>shows</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>simply</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word                                              Count\n",
       "0                [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, ...\n",
       "1         <UNK>  [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...\n",
       "2             5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...\n",
       "3        acting  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "4         after  [0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, ...\n",
       "..          ...                                                ...\n",
       "95         real  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "96    realistic  [0, 0, 0, 1, 0, 1, 2, 0, 0, 3, 0, 0, 0, 0, 0, ...\n",
       "97  recommended  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "98        shows  [0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
       "99       simply  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Encode reviews (train_reviews['Review_Idxs']) as bag-of-words vectors'''\n",
    "\n",
    "import numpy \n",
    "\n",
    "def idx_seqs_to_bows(idx_seqs, matrix_length):\n",
    "    #complete the function to return an array having bag-of-words vectors of the encoded reviews\n",
    "    # hint: numpy.bincount()\n",
    "    bow_seqs = np.bincount(idx_seqs)\n",
    "    return bow_seqs\n",
    "    \n",
    "\n",
    "#bow_train_reviews = idx_seqs_to_bows(train_reviews['Review_Idxs'],matrix_length=len(lexicon) + 1) #add one to length for padding)\n",
    "bow_train_reviews = train_reviews['Review_Idxs'].apply(lambda x:idx_seqs_to_bows(x,matrix_length=len(lexicon)+1))\n",
    "\n",
    "print(\"TRAIN INPUT:\\n\", bow_train_reviews)\n",
    "print(\"SHAPE:\", bow_train_reviews.shape, \"\\n\")\n",
    "\n",
    "#Showing an example mapping string words to counts\n",
    "lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "lexicon_lookup[0] = \"\"\n",
    "pd.DataFrame([(lexicon_lookup[idx], count) for idx, count in enumerate(bow_train_reviews)], columns=['Word', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Building a Recurrent Neural Network </font>\n",
    "\n",
    "\n",
    "\n",
    "###  <font color='#6629b2'>Numerical lists to matrices</font>\n",
    "\n",
    "The input representation for the RNN explicitly encodes the order of words in the review. We'll return to the lists of the word indices contained in train_reviews['Review_Idxs']. The input to the model will be these number sequences themselves. We need to put all the reviews in the training set into a single matrix, where each row is a review and each column is a word index in that sequence. This enables the model to process multiple sequences in parallel (batches) as opposed to one at a time. Using batches significantly speeds up training. However, each review has a different number of words, so we create a padded matrix equal to the length on the longest review in the training set. For all reviews with fewer words, we prepend the row with zeros representing an empty word position. This is why the number 0 was not assigned as a word index in the lexicon. We can tell Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INPUT:\n",
      " [[   0    0    0 ...   47    3   28]\n",
      " [   0    0    0 ...   83   63   59]\n",
      " [   0    0    0 ...   49  134   92]\n",
      " ...\n",
      " [   0    0    0 ...  129  804 1382]\n",
      " [   0    0    0 ...   54  705   41]\n",
      " [   0    0    0 ...  342  284 1288]]\n",
      "SHAPE: (100, 509) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs):\n",
    "    \n",
    "    #find the biggest review's length and save it in the variable below\n",
    "    max_seq_len = train_reviews['Review_Idxs'].str.len().max()\n",
    "    \n",
    "    #pad all these indexed reviews and return these padded sequences\n",
    "    #HINT: use pad_sequences function by keras\n",
    "    return pad_sequences(idx_seqs,maxlen = max_seq_len)\n",
    "\n",
    "\n",
    "train_padded_idxs = pad_idx_seqs(train_reviews['Review_Idxs'])\n",
    "test_padded_idxs = pad_idx_seqs(test_reviews['Review_Idxs'])\n",
    "\n",
    "print(\"TRAIN INPUT:\\n\", train_padded_idxs)\n",
    "print(\"SHAPE:\", train_padded_idxs.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Model Layers</font>\n",
    "The RNN will have four layers:\n",
    "\n",
    "**1. Input**: The input layer takes in the matrix of word indices.\n",
    "\n",
    "**2. Embedding**: A [layer](https://keras.io/layers/embeddings/) that converts integer word indices into distributed vector representations (embeddings), which were introduced above. The difference here is that rather than plugging in embeddings from a pretrained model as before, the word embeddings will be learned inside the model itself. Thus, the input to the model will be the word indices rather than their embeddings, and the embedding values will change as the model is trained. The mask_zero=True parameter in this layer indicates that values of 0 in the matrix (the padding) will be ignored by the model.\n",
    "\n",
    "**3. GRU**: A [recurrent (GRU) hidden layer](https://keras.io/layers/recurrent/), the central component of the model. As it observes each word in the review, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the review at that timepoint. There are a few architectures for this layer - we use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer (see the materials at the bottom for an explanation of the difference). This layer outputs the last hidden state of the sequence (i.e. the hidden representation of the review after its last word is observed).\n",
    "\n",
    "**4. Dense**: An output [layer](https://keras.io/layers/core/#dense) that predicts the rating for the review based on its GRU representation given by the previous layer. It has one dimension that contains a continuous value (the rating). Add a proper activation function.\n",
    "\n",
    "###  <font color='#6629b2'>Parameters</font>\n",
    "\n",
    "Our function for creating the RNN takes the following parameters:\n",
    "\n",
    "**n_input_nodes**: As with the standard bag-of-words MLP, this is the number of unique words in the lexicon, plus one to account for the padding represented by 0 values. This indicates the number of rows in the embedding layer, where each row corresponds to a word.\n",
    "\n",
    "**n_embedding_nodes**: the number of dimensions (units) in the embedding layer, which can be freely defined. Here, it is set to 300.\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the GRU hidden layer. Like the embedding layer, this can be freely chosen. Here, it is set to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "def create_rnn_model(n_input_nodes, n_embedding_nodes, n_hidden_nodes):\n",
    "    \n",
    "    #complete this function to create a model and compile it having the 4 layers listed above.\n",
    "    #Note: Layer 1 -  Technically the shape of this layer is (batch_size, len(train_padded_idxs)).\n",
    "    # However, both the batch size and the length of the input matrix can be inferred from the input at training time. \n",
    "    # The batch size is implicitly included in the shape of the input, so it does not need to \n",
    "    # be specified as a dimension of the input. None can be given as placeholder for the input matrix length.\n",
    "    # By defining it as None, the model is flexible in accepting inputs with different lengths.\n",
    "    model = tf.keras.Sequential()\n",
    "    input_layer = Input(shape=(None,))\n",
    "    model.add(input_layer)\n",
    "    # Layer 2\n",
    "    model.add(Embedding(input_dim=n_input_nodes,output_dim = n_embedding_nodes,mask_zero = True))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(GRU(units = n_hidden_nodes))\n",
    "    \n",
    "    # Layer 4\n",
    "    model.add(Dense(units = 1))\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = create_rnn_model(n_input_nodes=len(lexicon) + 1, n_embedding_nodes=300, n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.3346 - accuracy: 0.5900\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2844 - accuracy: 0.7200\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1373 - accuracy: 0.6800\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0985 - accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0323 - accuracy: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2917c485130>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Train the rnn_model using the padded sequences and y=train_reviews['Rating'].\n",
    "You need  to convert train_reviews['Rating'] to tensor before passing it as an argument\n",
    "Hint: tf.convert_to_tensor\n",
    "batch_size=20, epochs=5\n",
    "'''\n",
    "train_input = train_padded_idxs\n",
    "train_output = tf.convert_to_tensor(train_sentiment[['sentiment']].values,dtype = tf.int64)\n",
    "test_input  = test_padded_idxs\n",
    "test_output = tf.convert_to_tensor(test_sentiment[['sentiment']].values,dtype = tf.int64)\n",
    "\n",
    "rnn_model.fit(x =train_input,y=train_output,epochs = 5,batch_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Prediction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST INPUT:\n",
      " [[   0    0    0 ... 1256  460  639]\n",
      " [   0    0    0 ...  266  208   49]\n",
      " [   0    0    0 ...  398    6   63]\n",
      " ...\n",
      " [   0    0    0 ... 1351 1542   21]\n",
      " [   0    0    0 ...   78  443  189]\n",
      " [   0    0    0 ...   88  473   60]]\n",
      "SHAPE: (100, 509) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Put test reviews in padded matrix just how we did for train_reviews'''\n",
    "\n",
    "\n",
    "print(\"TEST INPUT:\\n\", test_padded_idxs)\n",
    "print(\"SHAPE:\", test_padded_idxs.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predict the ratings '''\n",
    "\n",
    "#Since ratings are integers, need to round predicted rating to nearest integer\n",
    "test_reviews['RNN_Pred_Rating'] = np.round(rnn_model.predict(test_padded_idxs)[:,0]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7960526315789465\n"
     ]
    }
   ],
   "source": [
    "'''Evaluate the model with R^2'''\n",
    "\n",
    "# print the r2 score\n",
    "from sklearn.metrics import r2_score \n",
    "score = r2_score(test_reviews['RNN_Pred_Rating'],test_output)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the full test dataset of 25,000 reviews, the $R^2$ for this model is 0.622525. So the RNN outperforms the continuous bag-of-words MLP as well as the standard bag-of-words approach.\n",
    "Your score might not be good because we're training on only 100-200 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Visualizing data inside the model</font>\n",
    "\n",
    "To help visualize the data representation inside the model, we can look at the output of each layer in a model individually. Keras' Functional API lets you derive a new model with the layers from an existing model, so you can define the output to be a layer below the output layer in the original model. Calling predict() on this new model will produce the output of that layer for a given input. Of course, glancing at the numbers by themselves doesn't provide any interpretation of what the model has learned (although there are opportunities to [interpret these values](https://medium.com/civis-analytics/interpreting-and-visualizing-neural-networks-for-text-processing-e9dff0da9c22), but seeing them verifies the model is just a series of transformations from one matrix to another. The model stores its layers as the list model.layers, and you can retrieve specific layer by its position index in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING LAYER OUTPUT SHAPE: (100, 500)\n",
      "[-7.65265897e-03  2.05201246e-02  3.19134295e-02  2.55749598e-02\n",
      "  2.64417864e-02  3.35756615e-02  9.38560162e-03  3.65877561e-02\n",
      " -2.04797462e-02  1.71994939e-02  2.92575974e-02  2.35721692e-02\n",
      "  7.47844297e-03  2.36763638e-02  1.45026566e-02  4.01279237e-03\n",
      "  2.36452334e-02 -4.30870727e-02 -1.62908621e-02 -7.24585131e-02\n",
      "  1.68756172e-02 -1.01558603e-02 -2.35274881e-02  1.67772342e-02\n",
      "  2.29001231e-02 -1.46655496e-02 -2.94298194e-02 -3.14796120e-02\n",
      " -5.44016156e-03 -1.17722312e-02  1.44683234e-02  3.08129862e-02\n",
      "  2.80372854e-02 -6.40035979e-03  5.68462769e-03  2.57684384e-02\n",
      "  2.00733989e-02  1.83834247e-02  8.38497840e-03  4.80674533e-03\n",
      "  1.95823051e-02 -2.14669341e-03  2.61424519e-02  2.48013567e-02\n",
      "  1.54345930e-02 -1.99551210e-02  1.07974280e-02  4.41645924e-03\n",
      " -2.51103193e-02  1.69071183e-02 -2.21852902e-02  4.72250432e-02\n",
      " -2.63532549e-02  1.53796393e-02  2.00189054e-02  7.15101184e-03\n",
      " -1.78908743e-02  2.01202556e-02 -2.34364867e-02  2.50642598e-02\n",
      " -2.45745648e-02  3.09015252e-02  5.06128930e-03  5.74701233e-03\n",
      "  3.27336304e-02 -3.53458337e-02 -6.10069036e-02  4.48583104e-02\n",
      " -2.09273957e-02 -1.05197495e-02  4.84153349e-03 -3.05838585e-02\n",
      "  9.51021351e-03  7.51088094e-03  1.34691950e-02 -6.36195093e-02\n",
      " -2.31965650e-02  3.31395939e-02 -6.69248123e-03 -9.64662759e-04\n",
      "  1.32556930e-02  1.43692987e-02  1.06989061e-02  4.01191600e-03\n",
      "  1.03035932e-02 -2.12045796e-02  7.65707530e-03  1.13958539e-02\n",
      "  1.22828986e-02 -2.28802525e-02 -2.98436601e-02 -2.05923487e-02\n",
      "  1.96026266e-02  2.50372738e-02  1.68088675e-02  7.43269455e-04\n",
      " -1.00194197e-02  2.84331404e-02 -2.09055562e-02  2.16452461e-02\n",
      "  1.16291028e-02 -5.69100166e-03 -1.59454402e-02  2.94269454e-02\n",
      " -4.33689430e-02 -2.35270578e-02  3.06740366e-02  2.64228061e-02\n",
      "  2.88081206e-02 -2.45479681e-02  1.07965423e-02 -9.33486223e-03\n",
      "  1.39829414e-02  3.37526016e-02 -1.99324042e-02  3.93393636e-02\n",
      "  2.26480030e-02 -3.89483087e-02 -4.16720007e-03 -1.85461231e-02\n",
      " -5.85207604e-02  3.06180678e-02  7.62636866e-03 -2.67231613e-02\n",
      "  3.68443653e-02 -2.53338413e-03 -4.49381769e-05 -2.83048619e-02\n",
      "  4.82474193e-02 -2.22383533e-03  8.48991890e-03 -2.71885507e-02\n",
      " -2.40818672e-02 -4.46100114e-03  5.72635531e-02  8.65772367e-02\n",
      " -1.53424507e-02 -1.94053333e-02 -6.56955317e-03  2.71333791e-02\n",
      " -2.73895897e-02 -3.09262518e-02  2.74549928e-02  3.33533287e-02\n",
      "  1.27063477e-02  3.33630815e-02 -1.27634918e-02 -1.80662591e-02\n",
      " -1.27940532e-03  2.41398606e-02 -3.40400040e-02  1.15438383e-02\n",
      "  1.59368888e-02  1.51339918e-04 -2.17284523e-02 -1.96051616e-02\n",
      "  4.16130498e-02  2.68133096e-02 -4.01772605e-03 -1.92302540e-02\n",
      "  2.65954733e-02  3.84472571e-02 -1.88684501e-02 -2.62768529e-02\n",
      "  8.33008997e-03  1.80336870e-02 -1.84809826e-02  4.24618134e-04\n",
      " -3.15972827e-02 -1.08143426e-02  2.14335578e-03 -1.08189424e-02\n",
      "  3.65657359e-02 -1.20737031e-03  1.00721031e-01 -3.51153538e-02\n",
      " -1.78492330e-02  1.80623550e-02 -2.26572435e-02  1.71407349e-02\n",
      "  1.42783821e-02 -1.65701564e-02  2.17232741e-02 -1.68555602e-02\n",
      "  4.10566572e-03  4.12451215e-02 -7.05929175e-02 -9.58508253e-03\n",
      " -3.01447529e-02 -2.05845870e-02  2.50406284e-03 -1.88304354e-02\n",
      " -1.29523855e-02 -1.24020930e-02 -5.08508980e-02  2.05768477e-02\n",
      " -1.68521442e-02  3.19812074e-02  2.80123428e-02  1.32923108e-02\n",
      " -1.83229148e-03 -8.73701833e-03 -1.69415660e-02 -2.16956269e-02\n",
      " -2.24590469e-02 -1.67309139e-02  3.68053317e-02  1.47329597e-03\n",
      " -3.33691575e-02  1.60478707e-02  6.41947240e-03 -1.64342094e-02\n",
      " -1.01579577e-02  2.88922116e-02 -1.28806755e-03 -2.56370101e-02\n",
      " -4.12296131e-03  2.35700682e-02 -5.41628897e-03 -1.23448279e-02\n",
      " -6.16579875e-02 -2.92759389e-04  8.65959190e-03  2.67754197e-02\n",
      "  1.18422760e-02  7.29985815e-03  2.90038250e-02 -2.15281472e-02\n",
      " -1.07703842e-02  1.28513835e-02  5.52255753e-03 -3.02936882e-02\n",
      " -3.86976488e-02  5.05336188e-03  1.52228270e-02 -6.30355347e-03\n",
      " -8.78250413e-03  7.01191928e-03 -1.55103998e-03 -3.61596532e-02\n",
      " -3.61471511e-02 -2.01285221e-02  1.03417169e-02 -5.05640842e-02\n",
      "  4.16285684e-03 -2.86806859e-02  4.92078736e-02  3.40923667e-02\n",
      " -2.81200260e-02  3.14760879e-02  1.79703794e-02  1.09670712e-02\n",
      "  3.78931500e-03 -3.07851806e-02 -1.92349497e-03 -7.83451088e-03\n",
      " -2.63285525e-02 -1.71964569e-03  2.28108950e-02  1.60994101e-02\n",
      " -2.84300894e-02  6.26667775e-03 -2.66499706e-02 -2.84755267e-02\n",
      " -3.25736851e-02 -8.73131771e-03 -2.42787525e-02 -1.38644986e-02\n",
      "  2.25555487e-02  1.65511575e-03 -1.42035652e-02  5.37436223e-03\n",
      " -3.66046838e-02 -1.15569793e-02 -1.40073933e-02 -3.09700780e-02\n",
      " -3.49095017e-02  4.29266766e-02 -2.31122375e-02  2.38599759e-02\n",
      " -1.48581844e-02 -1.14094783e-02  3.00953165e-04  1.40479021e-03\n",
      "  2.36573368e-02  4.07099389e-02  1.01764798e-01  7.97291622e-02\n",
      "  5.21542784e-03  1.09102000e-02 -9.65358876e-03  1.74987130e-03\n",
      "  1.52357630e-02  2.48476826e-02 -2.15094574e-02  4.35999185e-02\n",
      " -4.44849432e-02 -3.11196931e-02 -2.59923805e-02  2.69966125e-02\n",
      " -1.73755772e-02 -1.14413667e-02  1.76756047e-02 -1.48716969e-02\n",
      " -4.93473932e-02 -5.31737208e-02 -4.09878008e-02  8.65730271e-03\n",
      "  2.67717689e-02  2.21667215e-02 -3.16229239e-02  4.79751043e-02\n",
      " -8.16656742e-03  1.91676952e-02 -1.15057603e-02  8.07645917e-03\n",
      " -1.88794993e-02  3.75922173e-02 -4.22713496e-02 -2.29961779e-02\n",
      "  2.59585381e-02 -2.71580014e-02  2.40724348e-03 -6.08625868e-03\n",
      " -1.99759416e-02 -1.27773527e-02  2.31557190e-02 -2.29724720e-02\n",
      " -3.85876670e-02  3.30882072e-02  3.71564925e-03  2.24188007e-02\n",
      "  4.78306971e-03  1.31346919e-02 -2.36848369e-02  8.03939532e-03\n",
      " -2.23930366e-02 -2.35625692e-02  1.94678511e-02  1.29746208e-02\n",
      " -3.87527165e-03 -9.17181280e-03  2.35828292e-02 -2.55180560e-02\n",
      "  1.06435809e-02  1.82930045e-02  6.29506214e-03 -2.71557868e-02\n",
      " -8.22601002e-03  2.58719586e-02  1.79906618e-02 -1.44333988e-02\n",
      " -3.14783528e-02  3.76258194e-02 -2.02708170e-02  5.03848959e-03\n",
      "  2.31988430e-02 -1.68970898e-02 -3.12199555e-02  8.55619274e-03\n",
      " -2.90562436e-02  3.13778147e-02  2.46676840e-02  9.80141610e-02\n",
      "  2.82620899e-02 -1.05831614e-02 -5.93154244e-02  2.93526612e-02\n",
      " -2.55766008e-02 -1.57451537e-02 -2.13580430e-02 -4.23380248e-02\n",
      " -3.99748981e-03 -3.35088931e-02  1.47379106e-02  6.55974746e-02\n",
      " -1.95903778e-02  2.45557111e-02 -3.36864665e-02  4.35279496e-03\n",
      " -1.52682764e-02  1.26682147e-02 -2.96302103e-02 -1.33927166e-03\n",
      " -5.98760322e-03 -2.56891735e-02  5.83518157e-03  1.51051357e-02\n",
      "  5.81447687e-03  3.35485451e-02  2.00821161e-02 -1.13758445e-02\n",
      " -5.32697514e-03 -7.45553337e-03  1.46458913e-02  2.42786370e-02\n",
      " -4.80578467e-03 -1.96136981e-02  6.62672147e-03 -2.02238206e-02\n",
      " -1.20490240e-02  3.17935348e-02  3.52076516e-02  2.35083755e-02\n",
      "  2.93304790e-02 -2.50076167e-02 -4.76348447e-03  3.37022319e-02\n",
      " -2.07029246e-02 -3.06870863e-02 -2.29190923e-02  2.51201764e-02\n",
      " -3.04106474e-02 -3.60553935e-02 -2.68506352e-02  1.18903778e-02\n",
      "  6.14924356e-03 -2.94346288e-02 -3.06600425e-03 -2.05123238e-02\n",
      " -1.34830903e-02  1.82706234e-03 -2.96218656e-02  2.16736756e-02\n",
      "  2.28237994e-02 -2.58326065e-04 -8.59342143e-03  2.43649762e-02\n",
      "  9.97532997e-03 -2.63716653e-02 -2.74227373e-02  3.22130658e-02\n",
      " -1.48216151e-02 -4.67880182e-02 -1.80370081e-02  2.44292729e-02\n",
      " -9.89079475e-03  1.96458939e-02  1.88475214e-02 -2.54753735e-02\n",
      " -1.40657201e-02  2.70192195e-02  2.23227218e-02 -3.23240906e-02\n",
      "  2.59322431e-02 -3.29415239e-02  3.08139175e-02  3.78206894e-02\n",
      " -3.37082483e-02 -1.26858540e-02 -1.68991797e-02  2.39412077e-02\n",
      " -3.22667975e-03  1.04360273e-02  2.64390069e-03 -7.78621156e-03\n",
      "  2.03985116e-03  3.05105909e-03 -4.51822132e-02 -3.40644270e-02\n",
      " -2.68061236e-02 -7.02605676e-03  2.27483511e-02  1.51298419e-02\n",
      " -5.24142524e-04 -3.70429046e-02  3.69693805e-03  2.01170798e-02\n",
      " -4.36135195e-03 -1.80148762e-02  1.62778702e-02  2.92260610e-02\n",
      " -2.73881871e-02 -4.01716866e-02  7.85608869e-03 -4.96662874e-03\n",
      "  2.22710744e-02 -4.29859981e-02  2.10081507e-02 -2.85367612e-02\n",
      "  2.94887014e-02  2.17227302e-02 -3.06865461e-02 -3.90711762e-02\n",
      " -7.91663956e-03 -1.88210569e-02  1.99986491e-02 -6.74396195e-03\n",
      "  2.78628673e-02  3.05951275e-02  3.43283527e-02  4.37774602e-03\n",
      "  2.41049565e-02 -4.44389954e-02  3.07376571e-02 -2.83479877e-03\n",
      " -2.23600902e-02 -1.29835717e-02 -2.98793875e-02 -2.65992507e-02]\n"
     ]
    }
   ],
   "source": [
    "'''Showing the output of the RNN embedding layer (second layer) for the test reviews'''\n",
    "\n",
    "embedding_layer = Model(inputs=rnn_model.layers[0].input, \n",
    "                        outputs=rnn_model.layers[1].output) #embedding layer is 2nd layer (index 1)\n",
    "embedding_output = embedding_layer.predict(test_padded_idxs)\n",
    "print(\"EMBEDDING LAYER OUTPUT SHAPE:\", embedding_output.shape)\n",
    "print(embedding_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Conclusion</font>\n",
    "\n",
    "As mentioned above, the models shown here could be applied to any task where the goal is to predict a score for a particular sequence. For ratings prediction, this output is ordinal, but it could also be categorical with a few simple changes to the output layer of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color='#6629b2'>More resources</font>\n",
    "\n",
    "Yoav Goldberg's book [Neural Network Methods for Natural Language Processing](http://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037) is a thorough introduction to neural networks for NLP tasks in general.\n",
    "\n",
    "If you'd like to learn more about what Keras is doing under the hood, there is a [Theano tutorial](http://deeplearning.net/tutorial/lstm.html) that also applies an RNN to sentiment prediction, using the same dataset here\n",
    "\n",
    "Andrej Karpathy's blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is very helpful for understanding the mathematical details of an RNN, applied to the task of language modeling. It also provides raw Python code with an implementation of the backpropagation algorithm.\n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "Chris Olah provides a good [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTM RNNs work (this explanation also applies to the GRU model used here)\n",
    "\n",
    "Denny Britz's [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) documents well both the technical details of RNNs and their implementation in Python."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
