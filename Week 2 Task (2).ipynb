{"cells":[{"cell_type":"markdown","metadata":{"id":"2GKIJx3sYNC0"},"source":["# Task 2\n","This week you have learnt about various types of ML models. <br>\n","Let us focus on two of them."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26072,"status":"ok","timestamp":1633352328429,"user":{"displayName":"Amay Gada","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIegWflm0DVHy8h-sj9l7nvh_87WZPEby6oqxqjw=s64","userId":"18318224108445437394"},"user_tz":-330},"id":"PQ4lXvV6YW_5","outputId":"7b8f8554-2f9e-4a73-b378-dc3f2986812f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"markdown","metadata":{"id":"fKEbTpqVYg8m"},"source":["# Instructions\n","1. create a folder called synapse_w2 in your drive\n","2. add housing_data.csv, classified_data.txt, titanic_data.csv in the folder.\n","3. You will use the data from this path in this notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88aLM-5hYgVL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"yJqOW8lFYNC3"},"source":["# 1)  Linear Regression on Housing Price"]},{"cell_type":"markdown","metadata":{"id":"hZGyek2wYNC4"},"source":["### Import packages and dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C--rF9aZYNC4"},"outputs":[],"source":["# import numpy, pandas, matplotlib, seaborn\n","# add code here\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"P8Kgu-VJYNC5"},"source":["**Read housing_data.csv using pandas and call head() to show first few records.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrdEbQKxYNC5"},"outputs":[],"source":["# add code here\n","housing_data = pd.read_csv(\"housing_data.csv\")\n","housing_data.head()"]},{"cell_type":"markdown","metadata":{"id":"_158HqRTYNC6"},"source":["### Exloratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"YbmnOZ48YNC6"},"source":["**'info()' method to check the data types and number**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YWr6trZYNC6"},"outputs":[],"source":["# add code here\n","housing_data.info()"]},{"cell_type":"markdown","metadata":{"id":"-XVB-HteYNC7"},"source":["**Get the statistical summary of the data set** <br>\n","Hint: describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZPbIjgYYNC7"},"outputs":[],"source":["# add code here\n","housing_data.describe()"]},{"cell_type":"markdown","metadata":{"id":"DfSgsewYYNC7"},"source":["**Print the names of the columns(features)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ss5VF20YNC8"},"outputs":[],"source":["# add code here\n","print(list(housing_data.columns))"]},{"cell_type":"markdown","metadata":{"id":"p0HiN5NeYNC8"},"source":["### Basic plotting and visualization"]},{"cell_type":"markdown","metadata":{"id":"-rumpxTXYNC8"},"source":["**The target quantity is price. Let us see its distribution.** <br>\n","Plot a histogram of Price. Choose the number of bins by experimenting a little. (Expected: a bell curve shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BGkFMVBYNC8"},"outputs":[],"source":["# add code here\n","plt.hist(housing_data.Price, bins =80)"]},{"cell_type":"markdown","metadata":{"id":"Al174yCyYNC9"},"source":["**Let us see how the different features are correlated with each other by printing a Correlation Matrix**<br>\n","Hint: corr()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaLwISKZYNC9","scrolled":false},"outputs":[],"source":["# add code here\n","crM = housing_data.corr()\n","crM"]},{"cell_type":"markdown","metadata":{"id":"niwpNf_EYNC9"},"source":["### Feature and variable sets"]},{"cell_type":"markdown","metadata":{"id":"YZRq4NakYNC9"},"source":["**Make a list of data frame column names**\n","**Create a new dataframe containing all the numerical training features(note that Address is a string so ignore that) and store it in a variable called \"X\"**<br><br>\n","**Then create a new dataframe containing the target (Price) and store it in a variable called \"y\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBKubTuLYNC-"},"outputs":[],"source":["# add code here\n","X = housing_data.copy(deep = True)\n","X.drop('Address',axis =1, inplace = True)\n","y = pd.DataFrame(housing_data.Price)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2w-pU7UhYNC-"},"outputs":[],"source":["# This code should print (5000, 5) and (5000,) if everything is correct\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qC6JzmuDYNC-"},"outputs":[],"source":["# print a few record of X\n","# add code here\n","X.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQ9RkZ1-YNC-"},"outputs":[],"source":["# print a few record of y\n","# add code here\n","y.head()\n"]},{"cell_type":"markdown","metadata":{"id":"TvhXRmGyYNC_"},"source":["### Test-train split"]},{"cell_type":"markdown","metadata":{"id":"A-I6E4XuYNC_"},"source":["**Import train_test_split function from scikit-learn**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJ2t9WC4YNC_"},"outputs":[],"source":["# add code here\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"markdown","metadata":{"id":"wTHAGB7pYNC_"},"source":["**Create X and y train and test splits in one command using a test size of 0.3 and a random seed**<br>\n","They should be called X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZWMHfPcYNC_"},"outputs":[],"source":["# add code here\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0 )"]},{"cell_type":"markdown","metadata":{"id":"xyfldBi7YNDA"},"source":["**Print the size and shape of each of the train/test splits (it should be in the ratio as per test_size parameter above)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTMhv-xlYNDA"},"outputs":[],"source":["# add code here\n"]},{"cell_type":"markdown","metadata":{"id":"ZBLwVsQIYNDA"},"source":["### Model fit and training"]},{"cell_type":"markdown","metadata":{"id":"uNPr5Xb0YNDA"},"source":["**Import LinearRegression and metrics from scikit-learn**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llBzX5-EYNDA"},"outputs":[],"source":["# add code for imports here\n","\n","# Create a Linear Regression object 'lm' by calling LinearRegression()\n","# add code here\n","from sklearn.linear_model import LinearRegression\n","lm = LinearRegression()"]},{"cell_type":"markdown","metadata":{"id":"rRzoOJXhYNDA"},"source":["**Fit the model on to the instantiated object itself using the X_train and y_train created earlier. No need to create another variable**<br>\n","Hint: lm.fit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9U1pME1YNDB"},"outputs":[],"source":["# add code here\n","lm.fit(X_train.values, y_train.values)"]},{"cell_type":"markdown","metadata":{"id":"a7pxLfloYNDM"},"source":["### Prediction, error estimate, and regression evaluation matrices"]},{"cell_type":"markdown","metadata":{"id":"rOXcTK_iYNDN"},"source":["**Prediction using the lm model**<br>\n","Use model.predict() on X_test and store them in a variable called \"predictions\".<br>\n","Print type and size of the predictions. Size should be (1500,) if everything is correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ef28Cy7pYNDP"},"outputs":[],"source":["# add code here\n","prediction = lm.predict(X_test.values)\n","print(len(prediction))"]},{"cell_type":"markdown","metadata":{"id":"rPM6_2M5YNDQ"},"source":["**Since we're done with our predictions, let's compare it with y_test and see how accurate our predictions are.<br> Plot a Scatter plot of predicted price and y_test set to see if the data fall on a 45 degree straight line**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2T7YcpWMYNDQ","scrolled":false},"outputs":[],"source":["# add code here\n","fig, ax = plt.subplots(1,1,figsize = (20,10))\n","ax.scatter(prediction, y_test, color = 'r', label = 'Test Data')\n","ax.set_xlabel('Prediction')\n","ax.set_ylabel('y_test')"]},{"cell_type":"markdown","metadata":{"id":"LZtFB738YNDS"},"source":["**Print the R-square value and round it to 3 decimal places**<br>\n","Hint: sklearn metrics.r2_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGKgAkCDYNDS"},"outputs":[],"source":["# add code here\n","from sklearn.metrics import r2_score\n","score = r2_score(prediction,y_test)\n","print(round(score,3))"]},{"cell_type":"markdown","metadata":{"id":"2jroGWezYNDT"},"source":["# 2) K-nearest neighbor Classification"]},{"cell_type":"markdown","metadata":{"id":"WHlWn6YXYNDU"},"source":["### Import packages and dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJAjpGbYYNDU"},"outputs":[],"source":["# import numpy, pandas, matplotlib, seaborn\n","# add code here\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n"]},{"cell_type":"markdown","metadata":{"id":"8RWyQOGrYNDV"},"source":["**Read classified_data.txt using pandas and call head() to show first few records. Call this dataframe \"df\"** <br>\n","Use \"index_col\" parameter to index the dataframe according to the first column. Otherwise, a new column would get created."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GDXB_9GYNDV"},"outputs":[],"source":["# add code here\n","df = pd.read_csv(\"classified_data.txt\",index_col =[0])\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"QPOM6rXpYNDW"},"source":["### Exloratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"2J-RhwE5YNDW"},"source":["**'info()' method to check the data types and number**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQ27teSgYNDW"},"outputs":[],"source":["# add code here\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"lwSF-PxbYNDX"},"source":["**Get the statistical summary of the data set** <br>\n","Hint: describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7XvMQ5eYNDX"},"outputs":[],"source":["# add code here\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"LDYWYnvwYNDY"},"source":["### Check the spread of the features"]},{"cell_type":"markdown","metadata":{"id":"Q0hgD8wlYNDY"},"source":["**Store the column names in a list**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxAYeNSzYNDY"},"outputs":[],"source":["# add code here\n","cn = list(df.columns)"]},{"cell_type":"markdown","metadata":{"id":"N3g0ISUGYNDZ"},"source":["**Run a 'for' loop to draw boxplots of all the features for '0' and '1' TARGET CLASS**<br>\n","Hint: Loop through each of the 10 features and draw a separate boxplot. You should have 10 boxplots in total. <br>\n","Refer seaborn boxplot() "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kORQOo9gYNDZ","scrolled":false},"outputs":[],"source":["# add code here\n"]},{"cell_type":"markdown","metadata":{"id":"OIFaYgUWYNDa"},"source":["### Standardize the features using sklearn.preprocessing \n","Why should we standardize?<br>\n","Variables that are measured at different scales do not contribute equally to the model fitting & model learned function and might end up creating a bias. Thus, to deal with this potential problem feature-wise standardized (μ=0, σ=1) is usually used prior to model fitting.<br>\n","<br>\n","Go through this link for a better understanding:<br>\n","https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832"]},{"cell_type":"markdown","metadata":{"id":"EzMEGA9fYNDa"},"source":["**import StandardScaler from Sklearn and instantiate it to a variable called \"scaler\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVWU81uvYNDb"},"outputs":[],"source":["# add code here\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()"]},{"cell_type":"markdown","metadata":{"id":"HlKUjW9XYNDb"},"source":["**Fit only the features data to this scaler (leaving the TARGET CLASS column out) and then transform**<br>\n","Hint: scaler.fit() and scaler.transform()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCbkapp4YNDb"},"outputs":[],"source":["# add code here\n","scaler.fit(df)\n","scaled_data = scaler.transform(df)\n"]},{"cell_type":"markdown","metadata":{"id":"uZTbl8UBYNDc"},"source":["**Scaler.transform() will return an array. We need to convert this into a dataframe. Do this and add the column names to the dataframe. Call this new dataframe as \"df_feat\". Call head() on this df**<br>\n","Note: The final dataframe will have the initial columns except the \"TARGET CLASS\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6DnntqtYNDc"},"outputs":[],"source":["# add code here\n","df_feat = pd.DataFrame(scaled_data,columns = df.columns[::])\n","df_feat.head()"]},{"cell_type":"markdown","metadata":{"id":"vWiXgySxYNDd"},"source":["### Train/Test split"]},{"cell_type":"markdown","metadata":{"id":"xgJQPOfXYNDd"},"source":["**Set X to be equal to df_feat and set y accordingly. As you know, X contains our training features and y contains our target.**<br>\n","Hint: y can be taken directly from the initaial dataframe \"df\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W94s-2-jYNDd"},"outputs":[],"source":["# add code here\n","X= df_feat.copy(deep = True)\n","y = a"]},{"cell_type":"markdown","metadata":{"id":"5tyZdg0nYNDd"},"source":["**Import train_test_split function from scikit-learn**<br>\n","**Create X and y train and test splits in one command using a test size of 0.3 and a random seed**<br>\n","They should be called X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiNlmT2IYNDe"},"outputs":[],"source":["# add code here\n","X_train,X_test,y_train,y_test = train_test_split(X.values,y,test_size = 0.3, random_state = 0)"]},{"cell_type":"markdown","metadata":{"id":"N6tS1j4iYNDe"},"source":["### Model fit and training"]},{"cell_type":"markdown","metadata":{"id":"7A_LeaNAYNDe"},"source":["**import KNeighborsClassifier from sklearn and initialize it with neighbours = 1. Fit this on X_train and y_train**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaSJJTYDYNDf"},"outputs":[],"source":["# add code here\n","from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier(n_neighbors = 1)\n","knn.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{"id":"eAEVkt6LYNDf"},"source":["**Using this fitted model, predict on X_test. Store these predictions in variable called pred.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsnjjXaMYNDf"},"outputs":[],"source":["# add code here\n","pred = knn.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"7IcCOyuOYNDf"},"source":["**Let us check how correct these predictions are.<br>\n","Print a classification report of y_test and pred**<br>\n","Hint: sklearn classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC-20WeuYNDg"},"outputs":[],"source":["# add code here\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test,pred))"]},{"cell_type":"markdown","metadata":{"id":"exGvX0C5YNDg"},"source":["**Print the accuracy using numpy and round it to 3 decimal places.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSoOx6MiYNDg"},"outputs":[],"source":["# add code here\n","from sklearn.metrics import accuracy_score\n","accuracy = accuracy_score(y_test,pred)\n","print(round(accuracy,3))"]},{"cell_type":"markdown","metadata":{"id":"UWjjrBrbYNDh"},"source":["### Choosing optimal 'k'"]},{"cell_type":"markdown","metadata":{"id":"IT_5lBDVYNDh"},"source":["**Above, we chose n_neighbours to be equal to 1. Choosing a small value of K leads to unstable decision boundaries. <br>\n","We need to select n_neighbours by calculating the accuracy for every value of n from 1 to 60 and then choosing the one which gives the highest accuracy.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPKm3inZYNDh"},"outputs":[],"source":["# Do the same as we did above, but this time make a loop from n = 1 to n = 60 and append the accuracy\n","# for each in a list\n","\n","# add code here\n","list1 = []\n","for i in range(1,61):\n","    knn = KNeighborsClassifier(n_neighbors = i)\n","    knn.fit(X_train, y_train)\n","    pred = knn.predict(X_test)\n","    accuracy = accuracy_score(y_test,pred)\n","    list1.append(accuracy)\n"]},{"cell_type":"markdown","metadata":{"id":"5LUl_JMUYNDi"},"source":["**Plot a graph of K value vs Accuracy**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvsbINxsYNDi"},"outputs":[],"source":["# add code here\n","plt.plot(list1,list2,color = 'r', marker = 'o')\n","plt.xlabel('K value')\n","plt.ylabel('Accuracy')\n"]},{"cell_type":"markdown","metadata":{"id":"KO4KK1WEYNDi"},"source":["**Choose the best value of n_neighbours and give a reason why and also print the accuracy**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWf5hh-jYNDj"},"outputs":[],"source":["# add code here \n","knn = KNeighborsClassifier(n_neighbors = 35)\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","accuracy = accuracy_score(y_test,pred)\n","print(round(accuracy,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gIMbzLJd8bu"},"outputs":[],"source":["Any value of n_neighbours above 30 can be considered the best value as beyond that value accuracy always increases and is almost same."]},{"cell_type":"markdown","metadata":{"id":"z4CBOAiOd9is"},"source":["# 3). Decision Tree Classifier"]},{"cell_type":"markdown","metadata":{"id":"UXCHg4IQewph"},"source":["<b>read the titanic_data.csv using pandas and show the dataframe</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dGi9n5meB94"},"outputs":[],"source":["#write code here\n","titanic_data = pd.read_csv(\"titanic_data.csv\")"]},{"cell_type":"markdown","metadata":{"id":"ph135_ppe2I4"},"source":["<b>Write a function which accepts a dataframe, preprocesses the data (use task 1 notebook) and returns a new dataframe. </b> <br>\n","you may need a helper function for normalizing data so feel free to define that as well"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEgFQI3se0dn"},"outputs":[],"source":["def titanic_preprocessing_pipeline(df):\n","  def isNaN(value):\n","    try:\n","        import math\n","        return math.isnan(float(value))\n","    except:\n","        return False\n","    \n","    \n","def titanic_preprocessing_pipeline(df):\n","    import re \n","    name = list(df.Name)\n","    list2 = []\n","    for i in name:\n","        search = re.search('(\\w+)\\.',i)\n","        list2.append(search.group(1))\n","    df['Title'] = list2\n","    \n","    df.drop(['PassengerId','Ticket','Name'],axis = 'columns', inplace =True)\n","    \n","    nan_dict={}\n","    for j in df.columns:\n","        count=0\n","        total=0\n","        for i in range(0,891):\n","            if(isNaN(df[j][i])):\n","                count+=1\n","                total+=1\n","            else:\n","                total+=1\n","            percent = (count/total)*100\n","            nan_dict.update({j:percent})\n","            \n","    l = len(nan_dict)\n","    keys = list(nan_dict.keys())\n","    value = list(nan_dict.values())\n","    for i in range(0,l):\n","        if (value[i]>50):\n","            for j in range(0,l-1):\n","                if(keys[i]==df.columns[j]):\n","                    df.drop(df.columns[j], axis = 1,inplace =True)\n","                    \n","    mode = df.Embarked.mode()\n","    df.Embarked.fillna(f\"{mode}\", inplace = True)\n","    \n","    mean = df.Age.mean()\n","    df.Age.fillna(mean, inplace = True)\n","    \n","    fm_list =[]\n","    for i in range(0,891):\n","        fm_list.append(df.SibSp[i]+df.Parch[i])\n","    df['Family_members'] = fm_list\n","    \n","    df.drop(['Parch','SibSp'], inplace=True, axis=1)\n","    \n","    df.Age = (df.Age-df.Age.min())/(df.Age.max()-df.Age.min())\n","    df.Fare = (df.Age-df.Fare.min())/(df.Fare.max()-df.Fare.min())\n","    df.Family_members = (df.Age-df.Family_members.min())/(df.Family_members.max()-df.Family_members.min())\n","    \n","    dummy_Pclass = pd.get_dummies(df.Pclass)\n","    dummy_Sex = pd.get_dummies(df.Sex)\n","    dummy_Title = pd.get_dummies(df.Title)\n","    dummy_Embarked = pd.get_dummies(df.Embarked)\n","    \n","    df = pd.merge(left=df,right=dummy_Pclass,left_index=True,right_index=True)\n","    df = pd.merge(left=df,right=dummy_Sex,left_index=True,right_index=True)\n","    df = pd.merge(left=df,right=dummy_Title,left_index=True,right_index=True)\n","    df = pd.merge(left=df,right=dummy_Embarked,left_index=True,right_index=True)\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"Rlnz6P-xh1SA"},"source":["<b>extract the y label (survived) from the dataframe and store it in a new variable</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0WuPt4DhpOx"},"outputs":[],"source":["#write code here\n","y = new_df.pop('Survived')"]},{"cell_type":"markdown","metadata":{"id":"NzP6YHDTiC--"},"source":["<b>remove the y_label (survived) from the dataframe</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DI0KwhYHiIBT"},"outputs":[],"source":["#write code here"]},{"cell_type":"markdown","metadata":{"id":"7ut650pSiIw1"},"source":["<b>Split the data into train and test. (do a split in the ratio 30:70)</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phcn_hG7iRIN"},"outputs":[],"source":["#write code here\n","X_train,X_test,y_train,y_test = train_test_split(new_df,y,test_size=0.7,random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"sqZl02RsiSbJ"},"source":["<b>Now that you have the entire preprocessed and split data, implement the decision tree algorithm from sklearn and fit it to this dataset</b> <br>\n","\n","Make sure that you play with the hyperparameters to get a good result. You can even use bagging and boosting methods like random forest or adaboost to improve your accuracy. Visualize results, try different hyperparameters by using a loop, GET CREATIVE!<br>\n","\n","Machine learning is an iteritive process. You will have to keep playing with hyperparameters and algorithms. No fixed algorithm will work on a fixed dataset.\n","\n","Take this up as a challenge. The person with the best accuracy wins the round!"]},{"cell_type":"markdown","metadata":{"id":"EVNqRQx2j6iO"},"source":["<b>Note that the accuracy on the test set will be considered and brownie points for not overfitting the model in the process</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulNAgERBjytd"},"outputs":[],"source":["#write code here"]},{"cell_type":"markdown","metadata":{"id":"qpSDT-ZlkhRS"},"source":["<b>print the test accuracy and train accuracy here</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zUFG8T7kkJz"},"outputs":[],"source":["#write code here"]}],"metadata":{"colab":{"collapsed_sections":["niwpNf_EYNC9","TvhXRmGyYNC_","ZBLwVsQIYNDA","a7pxLfloYNDM","QPOM6rXpYNDW","LDYWYnvwYNDY","OIFaYgUWYNDa","vWiXgySxYNDd","N6tS1j4iYNDe"],"name":"Task2.ipynb","provenance":[]},"interpreter":{"hash":"63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"},"kernelspec":{"display_name":"Python 3.9.7 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":2}
